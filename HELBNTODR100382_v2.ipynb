{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HELBNTODR100382-v2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbXxcvycp+5+NSxyTHg+lF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sailkargutkar/R-Projects/blob/HELBNTODR100382/HELBNTODR100382_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdhg_2Kzzz1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "c0af37a1-359f-4a28-9de1-88f94bdf8e34"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import contractions\n",
        "import unicodedata\n",
        "import pathlib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.optimizers import Adamax, Adam\n",
        "from keras import layers\n",
        "from sklearn.metrics import f1_score, precision_score\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import GlobalMaxPooling1D,Conv1D,Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/00/92/a05b76a692ac08d470ae5c23873cf1c9a041532f1ee065e74b374f218306/contractions-0.0.25-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.7MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 21.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81702 sha256=5543866651a7a920a7e8bd5e882ddaabac9bab73348bac3c98c8edc5c671614d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.25 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5EQjBUE0Sou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, \n",
        "# semantic similarity, clustering, and other natural language tasks. \n",
        "# The pre-trained Universal Sentence Encoder is publicly available in Tensorflow-hub. \n",
        "# It comes with two variations i.e. one trained with Transformer encoder and other trained with Deep Averaging Network (DAN).\n",
        "universal_sentence_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS349hf90XeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"/content/agr_en_train.csv\")\n",
        "data.columns = ['Corpus','Text','Label']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWT94r4F1MN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e190c87f-5aff-4115-e955-40eece317635"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11998 entries, 0 to 11997\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Corpus  11998 non-null  object\n",
            " 1   Text    11998 non-null  object\n",
            " 2   Label   11998 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 281.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl7484UR1QN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataText = data['Text']\n",
        "dataLabel = data['Label']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzZ0MQ6h24NG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove all numbers from Text column\n",
        "data['Text'] = data['Text'].str.replace('\\d+', '')\n",
        "\n",
        "# Remove any type of alphanumeric character from our dataset.\n",
        "data.Text.str.replace('[^a-zA-Z0-9]', '')\n",
        "\n",
        "# Remove any commas, dollar symbol, double quotes, single quotes, question mark and single letter from our dataset.\n",
        "\n",
        "# Remove multiple space fromour dataset.\n",
        "data['Text'] = data['Text'].replace('\\s+', ' ', regex=True)\n",
        "data['Text'] = data['Text'].str.replace(',', '')\n",
        "data['Text'] = data['Text'].str.replace('$', '')\n",
        "data['Text'] = data['Text'].str.replace('\"', '')\n",
        "data['Text'] = data['Text'].str.replace('?', '')\n",
        "data['Text'] = data['Text'].str.replace(\"'\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
        "data['Text'] = data['Text'].str.replace(\"/\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(\"*\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(\"+\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(\":\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(\"(\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(\")\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(\"[\", \"\")\n",
        "data['Text'] = data['Text'].str.replace(\"]\", \"\")\n",
        "\n",
        "# Remove punctuation from our detaset.\n",
        "data['Text'] = data['Text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTg3wg7p3lkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = data['Text'].tolist()\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "train_label = np.asarray(pd.get_dummies(data.Label), dtype = np.int8)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejZAcEM637eX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "744e02b0-12a8-4d0a-c100-d0092cb360c3"
      },
      "source": [
        "train_label"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       ...,\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 1, 0]], dtype=int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRMK5kfFHTK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b01240ff-11b1-49e2-9f5f-e6e366b3c0b5"
      },
      "source": [
        "print(len(train_text))\n",
        "print(len(train_label))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11998\n",
            "11998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubdRFrwMLL4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_texts, test_texts, y_train, y_test =\\\n",
        "  train_test_split(\n",
        "    train_text,\n",
        "    train_label,\n",
        "    test_size=.1,\n",
        "    random_state=10\n",
        "  )"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JZKohfYN4DB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "b53efff9-c76a-49a0-8c81-c1de659e872d"
      },
      "source": [
        "print(train_texts.shape)\n",
        "print(test_texts.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "# y_train"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10798, 1)\n",
            "(1200, 1)\n",
            "(10798, 3)\n",
            "(1200, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTH8C3a3ONWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "b5d28168-a775-4a7b-aa81-3db628db83d7"
      },
      "source": [
        "train_texts"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Wise men dont speak at every conversation they speak the right words at the right time and he spoke wht is true'],\n",
              "       ['dnt teach ground realitythe govt hv to tell how much time they need to change the lawwe need action not assurance'],\n",
              "       ['Hi Sonia would like to know whether to hold Ashok Leyland or Sell it'],\n",
              "       ...,\n",
              "       ['First stop the black money in politics dont give election tickets to criminals Clean your own house first before trying to clean others house Gov has the list of names of people having money in Swiss banks and Panama files they are too big corporates who gives lots of their black money to all political parties including BJP No one can touch them Demonitization cleans the black money from middle class business and not from big corporates who hold assents in and '],\n",
              "       ['Tesla market cap more than FORD GM is it threat to Indian car maker also'],\n",
              "       ['Where is the outrage for deaths of our Jawans Where are offers of bangles for PM by BJP MPs Or dont they care now that they are in Govt']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3JmxMjvO1kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "ff559e22-e8f7-4cc7-a430-fd0f02574ff1"
      },
      "source": [
        "test_texts"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['No bdy knows about villagers they going to dead  why he is great pm give me reason'],\n",
              "       ['Demonitisation is historicand is best oppurtunity to take thecountry tofuture or to the botom its great oppurtunity historical'],\n",
              "       ['Congrats Rahul mummy will be soo happy with ur joke best comedian in the Parliment'],\n",
              "       ...,\n",
              "       ['If half of the amount was invested earlier for all the athletes then it surely fetches more no And golds too Ny ways good move'],\n",
              "       ['Move is gud but pl ensure atm has sufficient cash'],\n",
              "       ['When guys study Do really study ']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1SVm8IRHWG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9a17c7a5-e823-44b2-bb70-ba8cbebcf6b7"
      },
      "source": [
        "X_train = []\n",
        "for r in tqdm(train_texts):\n",
        "  emb = universal_sentence_encoder(r)\n",
        "  text_emb = tf.reshape(emb, [-1]).numpy()\n",
        "  X_train.append(text_emb)\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "X_test = []\n",
        "for r in tqdm(test_texts):\n",
        "  emb = universal_sentence_encoder(r)\n",
        "  text_emb = tf.reshape(emb, [-1]).numpy()\n",
        "  X_test.append(text_emb)\n",
        "X_test = np.array(X_test)\n",
        "print(X_train.shape, y_train.shape)\\"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10798/10798 [00:26<00:00, 400.11it/s]\n",
            "100%|██████████| 1200/1200 [00:02<00:00, 417.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(10798, 512) (10798, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGxTvf67MUtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42714e17-e588-42b2-c4b8-0e437cbff1d8"
      },
      "source": [
        "print(X_train.shape, y_train.shape)\\"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10798, 512) (10798, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL2CjT0IAiOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split training data and testing data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=10)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTzi5w254D5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A convolutional neural network (CNN) is a specific type of artificial neural network that uses perceptrons, \n",
        "# a machine learning unit algorithm, for supervised learning, \n",
        "# to analyze data. CNNs apply to image processing, natural language processing and other kinds of cognitive tasks.\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=10798, \n",
        "                           output_dim=200, \n",
        "                           input_length=512))\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=8, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='relu'))"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlYcaAGrAtwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "e2e43037-7297-47ef-871a-d1ed84957fe6"
      },
      "source": [
        "# summary() function is a generic function used to produce result summaries of the results of various model fitting functions. \n",
        "# The function invokes particular methods which depend on the class of the first argument.\n",
        "print(model.summary())"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 512, 200)          2159600   \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 505, 64)           102464    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 252, 64)           0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 16128)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                161290    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 2,423,387\n",
            "Trainable params: 2,423,387\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODhTaaeTAwj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function returns the specified source as a code object, ready to be executed\n",
        "# compiling out model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqVGckdBPEe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b23d3ed-1471-4e6e-982a-2c4bdb974790"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10798, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MeZuXlmPHhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "def96fc6-b2a5-4314-bb1b-67a624f04c04"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10798, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ECN_4gSAy0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "bdef8b51-2d18-4686-bdb9-cad5aa7389c3"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=16,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 9718 samples, validate on 1080 samples\n",
            "Epoch 1/3\n",
            "9718/9718 [==============================] - 104s 11ms/step - loss: 6.1253 - accuracy: 0.4164 - val_loss: 5.8762 - val_accuracy: 0.4537\n",
            "Epoch 2/3\n",
            "9718/9718 [==============================] - 102s 11ms/step - loss: 6.1245 - accuracy: 0.4189 - val_loss: 5.8745 - val_accuracy: 0.4537\n",
            "Epoch 3/3\n",
            "9718/9718 [==============================] - 102s 11ms/step - loss: 6.1245 - accuracy: 0.4189 - val_loss: 5.8751 - val_accuracy: 0.4537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLu5yKetP5LP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4d7ce332-a9bb-4df6-b249-408df738a37d"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1200/1200 [==============================] - 3s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.246852906545003, 0.40833333134651184]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z_gUXO1I1fX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "1311923d-94b4-46fb-c5f3-8489d9659a9b"
      },
      "source": [
        "y_pred = model.predict(X_train[:10])\n",
        "print(y_pred)\n",
        "\n",
        "# y_pred1  = model.predict(X_test)\n",
        "# y_pred1\n",
        "# y_pred2 = np.argmax(y_pred1, axis=1)\n",
        "# y_test2 = np.argmax(y_test, axis=1)\n",
        "\n",
        "# cm=confusion_matrix(y_test2,y_pred2)\n",
        "# print(cm)\n",
        "\n",
        "# y_pred1  = model.predict(X_test)\n",
        "# y_pred2 = np.argmax(y_pred1, axis=1)\n",
        "# y_test2 = np.argmax(y_test, axis=1)\n",
        "# print(y_pred1)\n",
        "# print(y_test)\n",
        "\n",
        "# score = f1_score(y_test2,y_pred2,average=\"micro\")\n",
        "# print(\"F1 score : \", score)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]\n",
            " [0.       4.062939 2.155662]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1hVOD4o8bqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}